"""
å®Œæ•´è¯„ä¼°æµç¨‹

è¿è¡Œå®Œæ•´çš„æ•°æ®ç”Ÿæˆå’Œè¯„ä¼°æµç¨‹ï¼š
1. ç”ŸæˆAIMEé¢˜ç›®
2. LLM Judgeè¯„ä¼°
3. Win Rateè¯„ä¼°
4. ç”Ÿæˆç»¼åˆæŠ¥å‘Š

è¿è¡Œæ–¹æ³•ï¼š
python data_generation/run_complete_evaluation.py 30 3.0

å‚æ•°ï¼š
- 30: ç”Ÿæˆé¢˜ç›®æ•°é‡
- 3.0: æ¯æ¬¡ç”Ÿæˆä¹‹é—´çš„å»¶è¿Ÿï¼ˆç§’ï¼‰

è¯´æ˜ï¼š
- ä½¿ç”¨AIME 2025å¹´çœŸé¢˜ä½œä¸ºå‚è€ƒ
- æ•°æ®é›†æ¥æºï¼šmath-ai/aime25ï¼ˆJSONLæ ¼å¼ï¼‰
"""

import json
import os
import sys
from datetime import datetime
from aime_generator import AIMEGenerator
from hello_agents import SimpleAgent, HelloAgentsLLM
from hello_agents.tools import LLMJudgeTool, WinRateTool


def run_complete_evaluation(
    num_problems: int = 30,
    delay_seconds: float = 3.0
):
    """
    è¿è¡Œå®Œæ•´è¯„ä¼°æµç¨‹

    Args:
        num_problems: ç”Ÿæˆé¢˜ç›®æ•°é‡
        delay_seconds: æ¯æ¬¡ç”Ÿæˆä¹‹é—´çš„å»¶è¿Ÿï¼ˆç§’ï¼‰ï¼Œé¿å…APIé€Ÿç‡é™åˆ¶
    """
    print("\n" + "="*80)
    print("ğŸš€ AIMEæ•°æ®ç”Ÿæˆä¸è¯„ä¼°å®Œæ•´æµç¨‹")
    print("="*80)
    print(f"\né…ç½®ä¿¡æ¯:")
    print(f"  - ç”Ÿæˆé¢˜ç›®æ•°é‡: {num_problems}")
    print(f"  - APIå»¶è¿Ÿ: {delay_seconds}ç§’/é¢˜")
    print(f"  - ç”Ÿæˆå‚è€ƒæ•°æ®: TianHongZXY/aime-1983-2025ï¼ˆ900+é“é¢˜ï¼‰")
    print(f"  - è¯„ä¼°å‚è€ƒ: AIME 2025çœŸé¢˜")

    # ========== æ­¥éª¤1: ç”ŸæˆAIMEé¢˜ç›® ==========
    print("\n" + "="*80)
    print("ğŸ“ æ­¥éª¤1: ç”ŸæˆAIMEé¢˜ç›®")
    print("="*80)

    generator = AIMEGenerator(delay_seconds=delay_seconds)
    generated_data_path = generator.generate_and_save(
        num_problems=num_problems,
        output_dir="data_generation/generated_data"
    )

    print(f"\nâœ… æ­¥éª¤1å®Œæˆï¼ç”Ÿæˆæ•°æ®ä¿å­˜åœ¨: {generated_data_path}")

    # ========== æ­¥éª¤2: è¯„ä¼° ==========
    # åˆ›å»ºè¯„ä¼°ç»“æœç›®å½•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    evaluation_dir = f"data_generation/evaluation_results/{timestamp}"
    os.makedirs(evaluation_dir, exist_ok=True)
    os.makedirs(os.path.join(evaluation_dir, "llm_judge"), exist_ok=True)
    os.makedirs(os.path.join(evaluation_dir, "win_rate"), exist_ok=True)

    # åˆ›å»ºLLM
    llm = HelloAgentsLLM()

    # ========== æ­¥éª¤2.1: LLM Judgeè¯„ä¼° ==========
    print(f"\nğŸ¯ æ­¥éª¤2.1: LLM Judgeè¯„ä¼° (vs AIME 2025)")

    llm_judge_result = None
    try:
        llm_judge_tool = LLMJudgeTool(llm=llm)

        llm_judge_result_json = llm_judge_tool.run({
            "generated_data_path": generated_data_path,
            "reference_year": 2025,
            "max_samples": num_problems,
            "output_dir": os.path.join(evaluation_dir, "llm_judge"),
            "judge_model": "gpt-4o"
        })

        llm_judge_result = json.loads(llm_judge_result_json)
        print(f"\nâœ… LLM Judgeè¯„ä¼°å®Œæˆï¼")
        print(f"   å¹³å‡æ€»åˆ†: {llm_judge_result['metrics']['average_total_score']:.2f}/5.0")
        print(f"   é€šè¿‡ç‡: {llm_judge_result['metrics']['pass_rate']:.2%}")
    except Exception as e:
        print(f"\nâŒ LLM Judgeè¯„ä¼°å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

    # ========== æ­¥éª¤2.2: Win Rateè¯„ä¼° ==========
    print(f"\nğŸ† æ­¥éª¤2.2: Win Rateè¯„ä¼° (vs AIME 2025)")

    win_rate_result = None
    try:
        win_rate_tool = WinRateTool(llm=llm)

        win_rate_result_json = win_rate_tool.run({
            "generated_data_path": generated_data_path,
            "reference_year": 2025,
            "num_comparisons": min(num_problems, 20),  # æœ€å¤š20æ¬¡å¯¹æ¯”
            "output_dir": os.path.join(evaluation_dir, "win_rate"),
            "judge_model": "gpt-4o"
        })

        win_rate_result = json.loads(win_rate_result_json)
        print(f"\nâœ… Win Rateè¯„ä¼°å®Œæˆï¼")
        print(f"   Win Rate: {win_rate_result['metrics']['win_rate']:.2%}")
    except Exception as e:
        print(f"\nâŒ Win Rateè¯„ä¼°å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

    # ========== æ­¥éª¤3: ç”Ÿæˆç»¼åˆæŠ¥å‘Š ==========
    comprehensive_report_path = None
    if llm_judge_result or win_rate_result:
        print("\n" + "="*80)
        print("ğŸ“Š æ­¥éª¤3: ç”Ÿæˆç»¼åˆæŠ¥å‘Š")
        print("="*80)

        comprehensive_report_path = os.path.join(evaluation_dir, "comprehensive_report.md")

        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        report = generate_comprehensive_report(
            generated_data_path,
            llm_judge_result,
            win_rate_result
        )

        with open(comprehensive_report_path, 'w', encoding='utf-8') as f:
            f.write(report)

        print(f"\nâœ… ç»¼åˆæŠ¥å‘Šå·²ä¿å­˜: {comprehensive_report_path}")

    # ========== å®Œæˆ ==========
    print("\n" + "="*80)
    print("ğŸ‰ å®Œæ•´è¯„ä¼°æµç¨‹å®Œæˆï¼")
    print("="*80)
    print(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
    print(f"   - ç”Ÿæˆæ•°æ®: {generated_data_path}")
    print(f"   - è¯„ä¼°ç»“æœç›®å½•: {evaluation_dir}")

    if llm_judge_result:
        print(f"   - LLM JudgeæŠ¥å‘Š: {llm_judge_result.get('report_file', 'N/A')}")
    if win_rate_result:
        print(f"   - Win RateæŠ¥å‘Š: {win_rate_result.get('report_file', 'N/A')}")

    if comprehensive_report_path:
        print(f"   - ç»¼åˆæŠ¥å‘Š: {comprehensive_report_path}")

    print(f"\nğŸ’¡ ä¸‹ä¸€æ­¥:")
    if comprehensive_report_path:
        print(f"   1. æŸ¥çœ‹ç»¼åˆæŠ¥å‘Š: {comprehensive_report_path}")
    print(f"   2. è¿è¡Œäººå·¥éªŒè¯: python data_generation/human_verification_ui.py {generated_data_path}")

    return {
        "generated_data_path": generated_data_path,
        "llm_judge_result": llm_judge_result,
        "win_rate_result": win_rate_result,
        "comprehensive_report_path": comprehensive_report_path
    }


def generate_comprehensive_report(
    generated_data_path: str,
    llm_judge_result: dict,
    win_rate_result: dict
) -> str:
    """ç”Ÿæˆç»¼åˆè¯„ä¼°æŠ¥å‘Š"""

    # åŠ è½½ç”Ÿæˆæ•°æ®
    with open(generated_data_path, 'r', encoding='utf-8') as f:
        generated_data = json.load(f)

    report = f"""# AIMEæ•°æ®ç”Ÿæˆä¸è¯„ä¼°ç»¼åˆæŠ¥å‘Š

## 1. åŸºæœ¬ä¿¡æ¯

- **ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
- **ç”Ÿæˆé¢˜ç›®æ•°é‡**: {len(generated_data)}
- **å‚è€ƒAIMEå¹´ä»½**: 2025
- **ç”Ÿæˆæ•°æ®è·¯å¾„**: {generated_data_path}

## 2. æ•°æ®ç”Ÿæˆç»Ÿè®¡

### ä¸»é¢˜åˆ†å¸ƒ

"""

    # ç»Ÿè®¡ä¸»é¢˜åˆ†å¸ƒ
    topic_counts = {}
    for item in generated_data:
        topic = item.get('topic', 'Unknown')
        topic_counts[topic] = topic_counts.get(topic, 0) + 1

    report += "| ä¸»é¢˜ | æ•°é‡ | å æ¯” |\n"
    report += "|------|------|------|\n"
    for topic, count in sorted(topic_counts.items(), key=lambda x: x[1], reverse=True):
        percentage = count / len(generated_data) * 100
        report += f"| {topic} | {count} | {percentage:.1f}% |\n"

    # LLM Judgeç»“æœ
    if llm_judge_result:
        report += "\n## 3. LLM Judgeè¯„ä¼°ç»“æœ\n\n"
        report += f"""**æ€»ä½“è¯„åˆ†**:
- å¹³å‡æ€»åˆ†: {llm_judge_result['metrics']['average_total_score']:.2f}/5.0
- é€šè¿‡ç‡: {llm_judge_result['metrics']['pass_rate']:.2%}
- ä¼˜ç§€ç‡: {llm_judge_result['metrics']['excellent_rate']:.2%}

**å„ç»´åº¦è¯„åˆ†**:

| ç»´åº¦ | å¹³å‡åˆ† |
|------|--------|
| æ­£ç¡®æ€§ | {llm_judge_result['metrics']['dimension_averages']['correctness']:.2f}/5.0 |
| æ¸…æ™°åº¦ | {llm_judge_result['metrics']['dimension_averages']['clarity']:.2f}/5.0 |
| éš¾åº¦åŒ¹é… | {llm_judge_result['metrics']['dimension_averages']['difficulty_match']:.2f}/5.0 |
| å®Œæ•´æ€§ | {llm_judge_result['metrics']['dimension_averages']['completeness']:.2f}/5.0 |

"""

    # Win Rateç»“æœ
    if win_rate_result:
        report += "\n## 4. Win Rateè¯„ä¼°ç»“æœ\n\n"
        report += f"""**èƒœç‡ç»Ÿè®¡**:
- Win Rate: {win_rate_result['metrics']['win_rate']:.2%}
- Loss Rate: {win_rate_result['metrics']['loss_rate']:.2%}
- Tie Rate: {win_rate_result['metrics']['tie_rate']:.2%}

**å¯¹æ¯”æ¬¡æ•°**:
- æ€»å¯¹æ¯”æ¬¡æ•°: {win_rate_result['metrics']['total_comparisons']} æ¬¡
- èƒœå‡ºæ¬¡æ•°: {win_rate_result['metrics']['wins']} æ¬¡
- å¤±è´¥æ¬¡æ•°: {win_rate_result['metrics']['losses']} æ¬¡
- å¹³å±€æ¬¡æ•°: {win_rate_result['metrics']['ties']} æ¬¡

"""

    # ç»¼åˆç»“è®º
    report += "\n## 5. ç»¼åˆç»“è®º\n\n"

    if llm_judge_result and win_rate_result:
        overall_avg_score = llm_judge_result['metrics']['average_total_score']
        overall_win_rate = win_rate_result['metrics']['win_rate']

        if overall_avg_score >= 4.5 and overall_win_rate >= 0.48:
            report += "âœ… **ç»“è®º**: ç”Ÿæˆæ•°æ®è´¨é‡**ä¼˜ç§€**ï¼Œè¾¾åˆ°æˆ–è¶…è¿‡AIMEçœŸé¢˜æ°´å¹³ã€‚\n"
        elif overall_avg_score >= 4.0 and overall_win_rate >= 0.45:
            report += "âœ… **ç»“è®º**: ç”Ÿæˆæ•°æ®è´¨é‡**è‰¯å¥½**ï¼Œæ¥è¿‘AIMEçœŸé¢˜æ°´å¹³ã€‚\n"
        else:
            report += "âš ï¸ **ç»“è®º**: ç”Ÿæˆæ•°æ®è´¨é‡**éœ€è¦æ”¹è¿›**ï¼Œä¸AIMEçœŸé¢˜ä»æœ‰å·®è·ã€‚\n"

        report += f"\n**æ•´ä½“æŒ‡æ ‡**:\n"
        report += f"- LLM Judgeå¾—åˆ†: {overall_avg_score:.2f}/5.0\n"
        report += f"- Win Rate: {overall_win_rate:.2%}\n"

    # æ”¹è¿›å»ºè®®
    report += "\n## 6. æ”¹è¿›å»ºè®®\n\n"

    if llm_judge_result:
        avg_score = llm_judge_result['metrics']['average_total_score']
        if avg_score >= 4.5:
            report += "- âœ… ç»§ç»­ä¿æŒå½“å‰çš„ç”Ÿæˆç­–ç•¥\n"
            report += "- âœ… å¯ä»¥è€ƒè™‘å¢åŠ ç”Ÿæˆæ•°é‡\n"
        elif avg_score >= 4.0:
            report += "- ğŸ”„ ä¼˜åŒ–é¢˜ç›®ç”Ÿæˆçš„æç¤ºè¯\n"
            report += "- ğŸ”„ å¢åŠ è´¨é‡è¿‡æ»¤æ­¥éª¤\n"
        else:
            report += "- âš ï¸ éœ€è¦é‡æ–°è®¾è®¡ç”Ÿæˆæç¤ºè¯\n"
            report += "- âš ï¸ è€ƒè™‘ä½¿ç”¨æ›´å¼ºçš„ç”Ÿæˆæ¨¡å‹\n"
            report += "- âš ï¸ å¢åŠ äººå·¥å®¡æ ¸ç¯èŠ‚\n"

    # ä¸‹ä¸€æ­¥è¡ŒåŠ¨
    report += "\n## 7. ä¸‹ä¸€æ­¥è¡ŒåŠ¨\n\n"
    report += "1. **äººå·¥éªŒè¯**: è¿è¡Œäººå·¥éªŒè¯ç•Œé¢ï¼Œå¯¹ç”Ÿæˆçš„é¢˜ç›®è¿›è¡Œäººå·¥å®¡æ ¸\n"
    report += f"   ```bash\n   python data_generation/human_verification_ui.py {generated_data_path}\n   ```\n\n"
    report += "2. **è´¨é‡ç­›é€‰**: æ ¹æ®è¯„ä¼°ç»“æœç­›é€‰é«˜è´¨é‡é¢˜ç›®\n\n"
    report += "3. **è¿­ä»£ä¼˜åŒ–**: æ ¹æ®è¯„ä¼°åé¦ˆä¼˜åŒ–ç”Ÿæˆç­–ç•¥\n"

    report += f"\n---\n\n*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n"

    return report


def main():
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python run_complete_evaluation.py <num_problems> [delay_seconds]")
        print("\nè¯´æ˜:")
        print("  - ä½¿ç”¨AIME 2025å¹´çœŸé¢˜ä½œä¸ºå‚è€ƒ")
        print("  - æ•°æ®é›†æ¥æº: math-ai/aime25ï¼ˆJSONLæ ¼å¼ï¼‰")
        print("\nç¤ºä¾‹:")
        print("python run_complete_evaluation.py 30 3.0")
        sys.exit(1)

    # è§£æå‘½ä»¤è¡Œå‚æ•°
    num_problems = int(sys.argv[1])
    delay_seconds = float(sys.argv[2]) if len(sys.argv) > 2 else 3.0

    # è¿è¡Œå®Œæ•´è¯„ä¼°
    run_complete_evaluation(
        num_problems=num_problems,
        delay_seconds=delay_seconds
    )


if __name__ == "__main__":
    main()

