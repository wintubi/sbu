"""
ç¤ºä¾‹7: æ¨¡å‹è¯„ä¼°

æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨RLTrainingToolè¯„ä¼°è®­ç»ƒåçš„æ¨¡å‹
"""

import sys
from pathlib import Path
import json

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent / "HelloAgents"
sys.path.insert(0, str(project_root))

from hello_agents.tools import RLTrainingTool


# ============================================================================
# ç¤ºä¾‹1: è¯„ä¼°SFTæ¨¡å‹
# ============================================================================

def evaluate_sft_model():
    """
    è¯„ä¼°SFTè®­ç»ƒåçš„æ¨¡å‹
    
    ä½¿ç”¨æµ‹è¯•é›†è¯„ä¼°æ¨¡å‹çš„å‡†ç¡®ç‡
    """
    tool = RLTrainingTool()
    
    config = {
        "action": "evaluate",
        "model_path": "./output/quick_test/sft",
        "max_samples": 50  # ä½¿ç”¨50ä¸ªæµ‹è¯•æ ·æœ¬
    }
    
    print("è¯„ä¼°SFTæ¨¡å‹:")
    print(f"  æ¨¡å‹è·¯å¾„: {config['model_path']}")
    print(f"  æµ‹è¯•æ ·æœ¬æ•°: {config['max_samples']}")
    
    # å®é™…è¯„ä¼°æ—¶å–æ¶ˆæ³¨é‡Š
    # result = tool.run(config)
    # result_dict = json.loads(result)
    # print(f"\nâœ… è¯„ä¼°å®Œæˆ!")
    # print(f"  å‡†ç¡®ç‡: {result_dict['accuracy']}")
    # print(f"  å¹³å‡å¥–åŠ±: {result_dict['average_reward']}")
    
    print("\nğŸ’¡ æç¤º: å–æ¶ˆæ³¨é‡Šä»¥è¿è¡Œè¯„ä¼°")
    
    return config


# ============================================================================
# ç¤ºä¾‹2: è¯„ä¼°GRPOæ¨¡å‹
# ============================================================================

def evaluate_grpo_model():
    """
    è¯„ä¼°GRPOè®­ç»ƒåçš„æ¨¡å‹
    
    å¯¹æ¯”GRPOæ¨¡å‹å’ŒSFTæ¨¡å‹çš„æ€§èƒ½
    """
    tool = RLTrainingTool()
    
    config = {
        "action": "evaluate",
        "model_path": "./output/quick_test/grpo",
        "max_samples": 50
    }
    
    print("è¯„ä¼°GRPOæ¨¡å‹:")
    print(f"  æ¨¡å‹è·¯å¾„: {config['model_path']}")
    print(f"  æµ‹è¯•æ ·æœ¬æ•°: {config['max_samples']}")
    
    # å®é™…è¯„ä¼°æ—¶å–æ¶ˆæ³¨é‡Š
    # result = tool.run(config)
    # result_dict = json.loads(result)
    # print(f"\nâœ… è¯„ä¼°å®Œæˆ!")
    # print(f"  å‡†ç¡®ç‡: {result_dict['accuracy']}")
    # print(f"  å¹³å‡å¥–åŠ±: {result_dict['average_reward']}")
    
    print("\nğŸ’¡ æç¤º: å–æ¶ˆæ³¨é‡Šä»¥è¿è¡Œè¯„ä¼°")
    
    return config


# ============================================================================
# ç¤ºä¾‹3: å¯¹æ¯”SFTå’ŒGRPOæ¨¡å‹
# ============================================================================

def compare_sft_grpo():
    """
    å¯¹æ¯”SFTå’ŒGRPOæ¨¡å‹çš„æ€§èƒ½
    
    åœ¨ç›¸åŒçš„æµ‹è¯•é›†ä¸Šè¯„ä¼°ä¸¤ä¸ªæ¨¡å‹
    """
    tool = RLTrainingTool()
    
    print("="*80)
    print("SFT vs GRPO æ¨¡å‹å¯¹æ¯”")
    print("="*80)
    
    # è¯„ä¼°SFTæ¨¡å‹
    print("\n1. è¯„ä¼°SFTæ¨¡å‹...")
    sft_config = {
        "action": "evaluate",
        "model_path": "./output/quick_test/sft",
        "max_samples": 100
    }
    
    # å®é™…è¯„ä¼°æ—¶å–æ¶ˆæ³¨é‡Š
    # sft_result = tool.run(sft_config)
    # sft_data = json.loads(sft_result)
    # print(f"   SFTå‡†ç¡®ç‡: {sft_data['accuracy']}")
    
    # è¯„ä¼°GRPOæ¨¡å‹
    print("\n2. è¯„ä¼°GRPOæ¨¡å‹...")
    grpo_config = {
        "action": "evaluate",
        "model_path": "./output/quick_test/grpo",
        "max_samples": 100
    }
    
    # å®é™…è¯„ä¼°æ—¶å–æ¶ˆæ³¨é‡Š
    # grpo_result = tool.run(grpo_config)
    # grpo_data = json.loads(grpo_result)
    # print(f"   GRPOå‡†ç¡®ç‡: {grpo_data['accuracy']}")
    
    # å¯¹æ¯”ç»“æœ
    print("\nå¯¹æ¯”ç»“æœ:")
    print("  SFTæ¨¡å‹: å­¦ä¹ åŸºæœ¬æ ¼å¼å’Œæ¨ç†æ­¥éª¤")
    print("  GRPOæ¨¡å‹: é€šè¿‡å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æ¨ç†èƒ½åŠ›")
    print("  é¢„æœŸ: GRPOæ¨¡å‹å‡†ç¡®ç‡ > SFTæ¨¡å‹å‡†ç¡®ç‡")
    
    print("\nğŸ’¡ æç¤º: å–æ¶ˆæ³¨é‡Šä»¥è¿è¡Œå®é™…è¯„ä¼°")
    
    return sft_config, grpo_config


# ============================================================================
# ç¤ºä¾‹4: è¯„ä¼°åŸºçº¿æ¨¡å‹
# ============================================================================

def evaluate_baseline():
    """
    è¯„ä¼°åŸºçº¿æ¨¡å‹(æœªè®­ç»ƒçš„åŸå§‹æ¨¡å‹)
    
    ç”¨äºå¯¹æ¯”è®­ç»ƒæ•ˆæœ
    """
    tool = RLTrainingTool()
    
    config = {
        "action": "evaluate",
        "model_path": "Qwen/Qwen3-0.6B",  # åŸå§‹æ¨¡å‹
        "max_samples": 50
    }
    
    print("è¯„ä¼°åŸºçº¿æ¨¡å‹:")
    print(f"  æ¨¡å‹: {config['model_path']}")
    print(f"  æµ‹è¯•æ ·æœ¬æ•°: {config['max_samples']}")
    
    # å®é™…è¯„ä¼°æ—¶å–æ¶ˆæ³¨é‡Š
    # result = tool.run(config)
    # result_dict = json.loads(result)
    # print(f"\nâœ… è¯„ä¼°å®Œæˆ!")
    # print(f"  åŸºçº¿å‡†ç¡®ç‡: {result_dict['accuracy']}")
    
    print("\nğŸ’¡ æç¤º: åŸºçº¿æ¨¡å‹é€šå¸¸å‡†ç¡®ç‡è¾ƒä½")
    print("   è®­ç»ƒåçš„æ¨¡å‹åº”è¯¥æ˜¾è‘—ä¼˜äºåŸºçº¿")
    
    return config


# ============================================================================
# ç¤ºä¾‹5: å®Œæ•´è¯„ä¼°æµç¨‹
# ============================================================================

def complete_evaluation():
    """
    å®Œæ•´çš„è¯„ä¼°æµç¨‹
    
    è¯„ä¼°åŸºçº¿ã€SFTå’ŒGRPOä¸‰ä¸ªæ¨¡å‹
    """
    tool = RLTrainingTool()
    
    models = {
        "åŸºçº¿æ¨¡å‹": "Qwen/Qwen3-0.6B",
        "SFTæ¨¡å‹": "./output/quick_test/sft",
        "GRPOæ¨¡å‹": "./output/quick_test/grpo"
    }
    
    print("="*80)
    print("å®Œæ•´è¯„ä¼°æµç¨‹")
    print("="*80)
    
    results = {}
    
    for name, model_path in models.items():
        print(f"\nè¯„ä¼° {name}...")
        print(f"  è·¯å¾„: {model_path}")
        
        config = {
            "action": "evaluate",
            "model_path": model_path,
            "max_samples": 100
        }
        
        # å®é™…è¯„ä¼°æ—¶å–æ¶ˆæ³¨é‡Š
        # result = tool.run(config)
        # result_dict = json.loads(result)
        # results[name] = result_dict
        # print(f"  å‡†ç¡®ç‡: {result_dict['accuracy']}")
    
    print("\n" + "="*80)
    print("è¯„ä¼°æ€»ç»“")
    print("="*80)
    
    # å®é™…è¯„ä¼°æ—¶å–æ¶ˆæ³¨é‡Š
    # for name, result in results.items():
    #     print(f"{name}: {result['accuracy']}")
    
    print("\né¢„æœŸç»“æœ:")
    print("  åŸºçº¿æ¨¡å‹ < SFTæ¨¡å‹ < GRPOæ¨¡å‹")
    print("  è¯´æ˜å¼ºåŒ–å­¦ä¹ è®­ç»ƒæœ‰æ•ˆæå‡äº†æ¨¡å‹æ€§èƒ½")
    
    print("\nğŸ’¡ æç¤º: å–æ¶ˆæ³¨é‡Šä»¥è¿è¡Œå®Œæ•´è¯„ä¼°")
    
    return models


# ============================================================================
# ç¤ºä¾‹6: å®é™…è¯„ä¼°ç¤ºä¾‹
# ============================================================================

def practical_evaluation():
    """
    å®é™…è¯„ä¼°ç¤ºä¾‹ - å¯ä»¥ç›´æ¥è¿è¡Œ
    
    è¯„ä¼°quick_testè®­ç»ƒçš„æ¨¡å‹
    """
    tool = RLTrainingTool()
    
    print("="*80)
    print("å®é™…è¯„ä¼°ç¤ºä¾‹")
    print("="*80)
    
    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
    import os
    sft_path = "./output/quick_test/sft"
    grpo_path = "./output/quick_test/grpo"
    
    if not os.path.exists(sft_path):
        print(f"\nâŒ SFTæ¨¡å‹ä¸å­˜åœ¨: {sft_path}")
        print("   è¯·å…ˆè¿è¡Œ 00_quick_test.py è®­ç»ƒæ¨¡å‹")
        return None
    
    if not os.path.exists(grpo_path):
        print(f"\nâŒ GRPOæ¨¡å‹ä¸å­˜åœ¨: {grpo_path}")
        print("   è¯·å…ˆè¿è¡Œ 00_quick_test.py è®­ç»ƒæ¨¡å‹")
        return None
    
    print("\nâœ… æ¨¡å‹æ–‡ä»¶å­˜åœ¨,å¼€å§‹è¯„ä¼°...")
    
    # è¯„ä¼°SFTæ¨¡å‹
    print("\n1. è¯„ä¼°SFTæ¨¡å‹...")
    sft_config = {
        "action": "evaluate",
        "model_path": sft_path,
        "max_samples": 20  # ä½¿ç”¨è¾ƒå°‘æ ·æœ¬å¿«é€Ÿæµ‹è¯•
    }
    
    print("ğŸ’¡ æç¤º: å–æ¶ˆä¸‹é¢çš„æ³¨é‡Šä»¥å¼€å§‹è¯„ä¼°")
    print("# sft_result = tool.run(sft_config)")
    print("# sft_data = json.loads(sft_result)")
    print("# print(f'SFTå‡†ç¡®ç‡: {sft_data[\"accuracy\"]}')")
    
    # è¯„ä¼°GRPOæ¨¡å‹
    print("\n2. è¯„ä¼°GRPOæ¨¡å‹...")
    grpo_config = {
        "action": "evaluate",
        "model_path": grpo_path,
        "max_samples": 20
    }
    
    print("ğŸ’¡ æç¤º: å–æ¶ˆä¸‹é¢çš„æ³¨é‡Šä»¥å¼€å§‹è¯„ä¼°")
    print("# grpo_result = tool.run(grpo_config)")
    print("# grpo_data = json.loads(grpo_result)")
    print("# print(f'GRPOå‡†ç¡®ç‡: {grpo_data[\"accuracy\"]}')")
    
    # å®é™…è¯„ä¼°æ—¶å–æ¶ˆæ³¨é‡Š
    # sft_result = tool.run(sft_config)
    # sft_data = json.loads(sft_result)
    # print(f"\nâœ… SFTè¯„ä¼°å®Œæˆ: {sft_data['accuracy']}")
    
    # grpo_result = tool.run(grpo_config)
    # grpo_data = json.loads(grpo_result)
    # print(f"âœ… GRPOè¯„ä¼°å®Œæˆ: {grpo_data['accuracy']}")
    
    return sft_config, grpo_config


# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================

if __name__ == "__main__":
    print("="*80)
    print("ç¤ºä¾‹1: è¯„ä¼°SFTæ¨¡å‹")
    print("="*80)
    evaluate_sft_model()
    
    print("\n" + "="*80)
    print("ç¤ºä¾‹2: è¯„ä¼°GRPOæ¨¡å‹")
    print("="*80)
    evaluate_grpo_model()
    
    print("\n" + "="*80)
    print("ç¤ºä¾‹3: å¯¹æ¯”SFTå’ŒGRPOæ¨¡å‹")
    print("="*80)
    compare_sft_grpo()
    
    print("\n" + "="*80)
    print("ç¤ºä¾‹4: è¯„ä¼°åŸºçº¿æ¨¡å‹")
    print("="*80)
    evaluate_baseline()
    
    print("\n" + "="*80)
    print("ç¤ºä¾‹5: å®Œæ•´è¯„ä¼°æµç¨‹")
    print("="*80)
    complete_evaluation()
    
    print("\n" + "="*80)
    print("ç¤ºä¾‹6: å®é™…è¯„ä¼°ç¤ºä¾‹")
    print("="*80)
    practical_evaluation()

