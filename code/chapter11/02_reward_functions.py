"""
ç¤ºä¾‹2: å¥–åŠ±å‡½æ•°è®¾è®¡å’Œä½¿ç”¨
æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨RLTrainingToolåˆ›å»ºå’Œæµ‹è¯•å¥–åŠ±å‡½æ•°
"""

import sys
from pathlib import Path
import json

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent / "HelloAgents"
sys.path.insert(0, str(project_root))

from hello_agents.tools import RLTrainingTool


# ============================================================================
# ç¤ºä¾‹1: åˆ›å»ºå‡†ç¡®æ€§å¥–åŠ±å‡½æ•°
# ============================================================================

def create_accuracy_reward():
    """
    åˆ›å»ºå‡†ç¡®æ€§å¥–åŠ±å‡½æ•°
    
    å¥–åŠ±è§„åˆ™:
    - ç­”æ¡ˆæ­£ç¡®: 1.0
    - ç­”æ¡ˆé”™è¯¯: 0.0
    """
    tool = RLTrainingTool()
    
    config = {
        "action": "create_reward",
        "reward_type": "accuracy"
    }
    
    print("åˆ›å»ºå‡†ç¡®æ€§å¥–åŠ±å‡½æ•°...")
    result = tool.run(config)
    result_dict = json.loads(result)
    
    print(f"âœ… å¥–åŠ±å‡½æ•°ç±»å‹: {result_dict['reward_type']}")
    print(f"ğŸ“‹ æè¿°: {result_dict['description']}")
    
    return result_dict


# ============================================================================
# ç¤ºä¾‹2: åˆ›å»ºé•¿åº¦æƒ©ç½šå¥–åŠ±å‡½æ•°
# ============================================================================

def create_length_penalty_reward():
    """
    åˆ›å»ºé•¿åº¦æƒ©ç½šå¥–åŠ±å‡½æ•°
    
    å¥–åŠ±è§„åˆ™:
    - åŸºç¡€å¥–åŠ± (å‡†ç¡®æ€§)
    - å‡å»é•¿åº¦æƒ©ç½š (é¼“åŠ±ç®€æ´)
    """
    tool = RLTrainingTool()
    
    config = {
        "action": "create_reward",
        "reward_type": "length_penalty",
        "penalty_weight": 0.001,  # æ¯ä¸ªtokenæƒ©ç½š0.001
        "max_length": 512
    }
    
    print("åˆ›å»ºé•¿åº¦æƒ©ç½šå¥–åŠ±å‡½æ•°...")
    result = tool.run(config)
    result_dict = json.loads(result)
    
    print(f"âœ… å¥–åŠ±å‡½æ•°ç±»å‹: {result_dict['reward_type']}")
    print(f"ğŸ“‹ æƒ©ç½šæƒé‡: {result_dict.get('penalty_weight', 0.001)}")
    print(f"ğŸ“‹ æœ€å¤§é•¿åº¦: {result_dict.get('max_length', 512)}")
    
    return result_dict


# ============================================================================
# ç¤ºä¾‹3: åˆ›å»ºæ­¥éª¤å¥–åŠ±å‡½æ•°
# ============================================================================

def create_step_reward():
    """
    åˆ›å»ºæ­¥éª¤å¥–åŠ±å‡½æ•°
    
    å¥–åŠ±è§„åˆ™:
    - åŸºç¡€å¥–åŠ± (å‡†ç¡®æ€§)
    - åŠ ä¸Šæ­¥éª¤å¥–åŠ± (é¼“åŠ±è¯¦ç»†æ¨ç†)
    """
    tool = RLTrainingTool()
    
    config = {
        "action": "create_reward",
        "reward_type": "step",
        "step_bonus": 0.1,  # æ¯ä¸ªæ­¥éª¤é¢å¤–å¥–åŠ±0.1
        "max_steps": 10
    }
    
    print("åˆ›å»ºæ­¥éª¤å¥–åŠ±å‡½æ•°...")
    result = tool.run(config)
    result_dict = json.loads(result)
    
    print(f"âœ… å¥–åŠ±å‡½æ•°ç±»å‹: {result_dict['reward_type']}")
    print(f"ğŸ“‹ æ­¥éª¤å¥–åŠ±: {result_dict.get('step_bonus', 0.1)}")
    print(f"ğŸ“‹ æœ€å¤§æ­¥éª¤: {result_dict.get('max_steps', 10)}")
    
    return result_dict


# ============================================================================
# ç¤ºä¾‹4: æµ‹è¯•å¥–åŠ±å‡½æ•°
# ============================================================================

def test_reward_function():
    """
    æµ‹è¯•å¥–åŠ±å‡½æ•°çš„è®¡ç®—
    
    ä½¿ç”¨MathRewardFunctionç›´æ¥æµ‹è¯•
    """
    from hello_agents.rl import MathRewardFunction
    
    reward_fn = MathRewardFunction(tolerance=1e-4)
    
    # æµ‹è¯•æ ·æœ¬
    test_cases = [
        {
            "completion": "Let me calculate: 2+2=4. Final Answer: 4",
            "ground_truth": "4",
            "expected": 1.0
        },
        {
            "completion": "I think 2+2=5. Final Answer: 5",
            "ground_truth": "4",
            "expected": 0.0
        },
        {
            "completion": "The answer is 4",
            "ground_truth": "4",
            "expected": 1.0
        },
        {
            "completion": "2+2 equals four. #### 4",
            "ground_truth": "4",
            "expected": 1.0
        }
    ]
    
    print("æµ‹è¯•å¥–åŠ±å‡½æ•°:")
    print("-" * 80)
    
    for i, case in enumerate(test_cases, 1):
        # è®¡ç®—å¥–åŠ±
        rewards = reward_fn(
            completions=[case["completion"]],
            ground_truth=[case["ground_truth"]]
        )
        reward = rewards[0]
        
        print(f"\næµ‹è¯• {i}:")
        print(f"  ç”Ÿæˆ: {case['completion'][:50]}...")
        print(f"  çœŸå€¼: {case['ground_truth']}")
        print(f"  å¥–åŠ±: {reward:.2f} (æœŸæœ›: {case['expected']:.2f})")
        print(f"  {'âœ… æ­£ç¡®' if abs(reward - case['expected']) < 0.01 else 'âŒ é”™è¯¯'}")
    
    return test_cases


# ============================================================================
# ç¤ºä¾‹5: ç­”æ¡ˆæå–æµ‹è¯•
# ============================================================================

def test_answer_extraction():
    """
    æµ‹è¯•ç­”æ¡ˆæå–åŠŸèƒ½
    """
    from hello_agents.rl import MathRewardFunction
    
    reward_fn = MathRewardFunction()
    
    test_texts = [
        "Final Answer: 42",
        "The answer is 3.14",
        "#### 100",
        "So the result is 2.5",
        "Let me think... the answer should be 7",
        "42"
    ]
    
    print("ç­”æ¡ˆæå–æµ‹è¯•:")
    print("-" * 80)
    
    for text in test_texts:
        answer = reward_fn.extract_answer(text)
        print(f"\næ–‡æœ¬: {text}")
        print(f"æå–: {answer if answer else '(æœªæ‰¾åˆ°)'}")
    
    return test_texts


# ============================================================================
# ç¤ºä¾‹6: ç­”æ¡ˆæ¯”è¾ƒæµ‹è¯•
# ============================================================================

def test_answer_comparison():
    """
    æµ‹è¯•ç­”æ¡ˆæ¯”è¾ƒåŠŸèƒ½
    """
    from hello_agents.rl import MathRewardFunction
    
    reward_fn = MathRewardFunction(tolerance=0.01)
    
    test_pairs = [
        ("42", "42", True),
        ("3.14", "3.14159", False),  # è¶…å‡ºå®¹å·®
        ("3.14", "3.141", True),     # åœ¨å®¹å·®å†…
        ("100", "100.0", True),
        ("2.5", "3.0", False),
        ("7", "7.00", True)
    ]
    
    print("ç­”æ¡ˆæ¯”è¾ƒæµ‹è¯•:")
    print("-" * 80)
    
    for pred, truth, expected in test_pairs:
        is_correct = reward_fn.compare_answers(pred, truth)
        print(f"\né¢„æµ‹: {pred}, çœŸå€¼: {truth}")
        print(f"ç»“æœ: {'æ­£ç¡®' if is_correct else 'é”™è¯¯'} (æœŸæœ›: {'æ­£ç¡®' if expected else 'é”™è¯¯'})")
        print(f"{'âœ… é€šè¿‡' if is_correct == expected else 'âŒ å¤±è´¥'}")
    
    return test_pairs


# ============================================================================
# ç¤ºä¾‹7: ä¸åŒå¥–åŠ±å‡½æ•°çš„å¯¹æ¯”
# ============================================================================

def compare_reward_functions():
    """
    å¯¹æ¯”ä¸åŒå¥–åŠ±å‡½æ•°çš„æ•ˆæœ
    """
    from hello_agents.rl import (
        create_accuracy_reward,
        create_length_penalty_reward,
        create_step_reward
    )

    # åˆ›å»ºä¸åŒçš„å¥–åŠ±å‡½æ•°
    accuracy_fn = create_accuracy_reward()
    base_fn = create_accuracy_reward()  # åŸºç¡€å¥–åŠ±å‡½æ•°
    length_fn = create_length_penalty_reward(base_fn, penalty_weight=0.001)
    step_fn = create_step_reward(base_fn, step_bonus=0.1)
    
    # æµ‹è¯•æ ·æœ¬
    test_cases = [
        {
            "completion": "4",
            "ground_truth": "4",
            "desc": "ç®€æ´æ­£ç¡®ç­”æ¡ˆ"
        },
        {
            "completion": "Step 1: 2+2=4\nFinal Answer: 4",
            "ground_truth": "4",
            "desc": "å¸¦æ­¥éª¤çš„æ­£ç¡®ç­”æ¡ˆ"
        },
        {
            "completion": "Let me think... " * 20 + "Final Answer: 4",
            "ground_truth": "4",
            "desc": "å†—é•¿çš„æ­£ç¡®ç­”æ¡ˆ"
        }
    ]
    
    print("å¥–åŠ±å‡½æ•°å¯¹æ¯”:")
    print("=" * 80)
    
    for i, case in enumerate(test_cases, 1):
        print(f"\næµ‹è¯• {i}: {case['desc']}")
        print(f"é•¿åº¦: {len(case['completion'])} å­—ç¬¦")
        
        # è®¡ç®—ä¸åŒå¥–åŠ±
        acc_reward = accuracy_fn([case["completion"]], ground_truth=[case["ground_truth"]])[0]
        len_reward = length_fn([case["completion"]], ground_truth=[case["ground_truth"]])[0]
        step_reward = step_fn([case["completion"]], ground_truth=[case["ground_truth"]])[0]
        
        print(f"  å‡†ç¡®æ€§å¥–åŠ±: {acc_reward:.4f}")
        print(f"  é•¿åº¦æƒ©ç½šå¥–åŠ±: {len_reward:.4f}")
        print(f"  æ­¥éª¤å¥–åŠ±: {step_reward:.4f}")
    
    print("\nç»“è®º:")
    print("  - å‡†ç¡®æ€§å¥–åŠ±: åªå…³æ³¨ç­”æ¡ˆæ­£ç¡®æ€§")
    print("  - é•¿åº¦æƒ©ç½š: é¼“åŠ±ç®€æ´ç­”æ¡ˆ")
    print("  - æ­¥éª¤å¥–åŠ±: é¼“åŠ±è¯¦ç»†æ¨ç†")
    
    return test_cases


# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================

if __name__ == "__main__":
    print("="*80)
    print("ç¤ºä¾‹1: åˆ›å»ºå‡†ç¡®æ€§å¥–åŠ±å‡½æ•°")
    print("="*80)
    create_accuracy_reward()
    
    print("\n" + "="*80)
    print("ç¤ºä¾‹2: åˆ›å»ºé•¿åº¦æƒ©ç½šå¥–åŠ±å‡½æ•°")
    print("="*80)
    create_length_penalty_reward()
    
    print("\n" + "="*80)
    print("ç¤ºä¾‹3: åˆ›å»ºæ­¥éª¤å¥–åŠ±å‡½æ•°")
    print("="*80)
    create_step_reward()
    
    print("\n" + "="*80)
    print("ç¤ºä¾‹4: æµ‹è¯•å¥–åŠ±å‡½æ•°")
    print("="*80)
    test_reward_function()
    
    print("\n" + "="*80)
    print("ç¤ºä¾‹5: ç­”æ¡ˆæå–æµ‹è¯•")
    print("="*80)
    test_answer_extraction()
    
    print("\n" + "="*80)
    print("ç¤ºä¾‹6: ç­”æ¡ˆæ¯”è¾ƒæµ‹è¯•")
    print("="*80)
    test_answer_comparison()
    
    print("\n" + "="*80)
    print("ç¤ºä¾‹7: ä¸åŒå¥–åŠ±å‡½æ•°çš„å¯¹æ¯”")
    print("="*80)
    compare_reward_functions()

