"""
ç¤ºä¾‹5: GRPOè®­ç»ƒå®Œæ•´æµç¨‹

æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨RLTrainingToolè¿›è¡ŒGRPOå¼ºåŒ–å­¦ä¹ è®­ç»ƒ
"""

import sys
from pathlib import Path
import json

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent / "HelloAgents"
sys.path.insert(0, str(project_root))

from hello_agents.tools import RLTrainingTool


# ============================================================================
# ç¤ºä¾‹1: æœ€ç®€å•çš„GRPOè®­ç»ƒ
# ============================================================================

def minimal_grpo_training():
    """
    æœ€ç®€å•çš„GRPOè®­ç»ƒç¤ºä¾‹
    
    åªéœ€è¦è°ƒç”¨RLTrainingToolå³å¯
    """
    tool = RLTrainingTool()
    
    config = {
        "action": "train",
        "algorithm": "grpo",
        "model_name": "Qwen/Qwen3-0.6B",
        "output_dir": "./output/grpo_minimal",
        "max_samples": 10,
        "num_epochs": 1,
    }
    
    print("æœ€ç®€å•çš„GRPOè®­ç»ƒ:")
    print(f"  æ¨¡å‹: {config['model_name']}")
    print(f"  æ ·æœ¬æ•°: {config['max_samples']}")
    print(f"  è®­ç»ƒè½®æ•°: {config['num_epochs']}")
    
    # å®é™…è®­ç»ƒæ—¶å–æ¶ˆæ³¨é‡Š
    # result = tool.run(config)
    # result_dict = json.loads(result)
    # print(f"\nâœ… è®­ç»ƒå®Œæˆ! æ¨¡å‹ä¿å­˜åœ¨: {result_dict['output_dir']}")
    
    return config


# ============================================================================
# ç¤ºä¾‹2: æ ‡å‡†GRPOè®­ç»ƒé…ç½®
# ============================================================================

def standard_grpo_training():
    """
    æ ‡å‡†çš„GRPOè®­ç»ƒé…ç½®
    
    é€šå¸¸åœ¨SFTæ¨¡å‹åŸºç¡€ä¸Šè¿›è¡ŒGRPOè®­ç»ƒ
    """
    tool = RLTrainingTool()
    
    config = {
        "action": "train",
        "algorithm": "grpo",
        
        # æ¨¡å‹é…ç½® - å¯ä»¥ä½¿ç”¨SFTè®­ç»ƒåçš„æ¨¡å‹
        "model_name": "Qwen/Qwen3-0.6B",  # æˆ– "./output/sft_standard"
        "output_dir": "./output/grpo_standard",
        
        # æ•°æ®é…ç½®
        "max_samples": 500,  # GRPOé€šå¸¸ä½¿ç”¨è¾ƒå°‘æ ·æœ¬
        
        # è®­ç»ƒé…ç½®
        "num_epochs": 3,
        "batch_size": 2,  # GRPOéœ€è¦æ›´å¤šæ˜¾å­˜
        "learning_rate": 1e-5,  # æ¯”SFTå°10å€
        
        # LoRAé…ç½®
        "use_lora": True,
        "lora_r": 16,
        "lora_alpha": 32,
    }
    
    print("æ ‡å‡†GRPOè®­ç»ƒé…ç½®:")
    print(f"  æ¨¡å‹: {config['model_name']}")
    print(f"  æ ·æœ¬æ•°: {config['max_samples']}")
    print(f"  è®­ç»ƒè½®æ•°: {config['num_epochs']}")
    print(f"  batch_size: {config['batch_size']}")
    print(f"  learning_rate: {config['learning_rate']} (æ¯”SFTå°)")
    
    # å®é™…è®­ç»ƒæ—¶å–æ¶ˆæ³¨é‡Š
    # result = tool.run(config)
    # result_dict = json.loads(result)
    # print(f"\nâœ… GRPOè®­ç»ƒå®Œæˆ!")
    
    return config


# ============================================================================
# ç¤ºä¾‹3: å®Œæ•´æ•°æ®é›†è®­ç»ƒ
# ============================================================================

def full_dataset_training():
    """
    ä½¿ç”¨å®Œæ•´æ•°æ®é›†è¿›è¡ŒGRPOè®­ç»ƒ
    """
    tool = RLTrainingTool()
    
    config = {
        "action": "train",
        "algorithm": "grpo",
        "model_name": "Qwen/Qwen3-0.6B",
        "output_dir": "./output/grpo_full",
        
        # ä½¿ç”¨å…¨éƒ¨æ•°æ®
        "max_samples": None,  # None = ä½¿ç”¨å…¨éƒ¨æ•°æ®
        
        "num_epochs": 3,
        "batch_size": 2,
        "learning_rate": 1e-5,
        "use_lora": True,
        "lora_r": 16,
        "lora_alpha": 32,
    }
    
    print("å®Œæ•´æ•°æ®é›†GRPOè®­ç»ƒ:")
    print(f"  æ¨¡å‹: {config['model_name']}")
    print(f"  æ ·æœ¬æ•°: å…¨éƒ¨ (max_samples=None)")
    print(f"  è®­ç»ƒè½®æ•°: {config['num_epochs']}")
    print(f"  é¢„è®¡æ ·æœ¬æ•°: ~7500 (GSM8Kè®­ç»ƒé›†)")
    
    # å®é™…è®­ç»ƒæ—¶å–æ¶ˆæ³¨é‡Š
    # result = tool.run(config)
    
    return config


# ============================================================================
# ç¤ºä¾‹4: SFT + GRPOå®Œæ•´æµç¨‹
# ============================================================================

def complete_sft_grpo_pipeline():
    """
    å®Œæ•´çš„SFT + GRPOè®­ç»ƒæµç¨‹
    
    æ­¥éª¤:
    1. SFTè®­ç»ƒ - å­¦ä¹ åŸºæœ¬æ ¼å¼
    2. GRPOè®­ç»ƒ - ä¼˜åŒ–æ¨ç†èƒ½åŠ›
    """
    tool = RLTrainingTool()
    
    # æ­¥éª¤1: SFTè®­ç»ƒ
    print("æ­¥éª¤1: SFTè®­ç»ƒ")
    sft_config = {
        "action": "train",
        "algorithm": "sft",
        "model_name": "Qwen/Qwen3-0.6B",
        "output_dir": "./output/pipeline_sft",
        "max_samples": 1000,
        "num_epochs": 3,
        "batch_size": 4,
        "use_lora": True,
    }
    
    print(f"  æ¨¡å‹: {sft_config['model_name']}")
    print(f"  æ ·æœ¬æ•°: {sft_config['max_samples']}")
    
    # å®é™…è®­ç»ƒæ—¶å–æ¶ˆæ³¨é‡Š
    # sft_result = tool.run(sft_config)
    # print(f"âœ… SFTè®­ç»ƒå®Œæˆ: {sft_config['output_dir']}")
    
    # æ­¥éª¤2: GRPOè®­ç»ƒ
    print("\næ­¥éª¤2: GRPOè®­ç»ƒ")
    grpo_config = {
        "action": "train",
        "algorithm": "grpo",
        "model_name": "./output/pipeline_sft",  # ä½¿ç”¨SFTæ¨¡å‹
        "output_dir": "./output/pipeline_grpo",
        "max_samples": 500,
        "num_epochs": 3,
        "batch_size": 2,
        "learning_rate": 1e-5,
        "use_lora": True,
    }
    
    print(f"  åŸºç¡€æ¨¡å‹: {grpo_config['model_name']}")
    print(f"  æ ·æœ¬æ•°: {grpo_config['max_samples']}")
    
    # å®é™…è®­ç»ƒæ—¶å–æ¶ˆæ³¨é‡Š
    # grpo_result = tool.run(grpo_config)
    # print(f"âœ… GRPOè®­ç»ƒå®Œæˆ: {grpo_config['output_dir']}")
    
    print("\nğŸ’¡ æ¨èä½¿ç”¨GRPOæ¨¡å‹è¿›è¡Œæ¨ç†")
    
    return sft_config, grpo_config


# ============================================================================
# ç¤ºä¾‹5: ä¸åŒå¥–åŠ±å‡½æ•°çš„ä½¿ç”¨
# ============================================================================

def using_different_rewards():
    """
    GRPOé»˜è®¤ä½¿ç”¨å‡†ç¡®æ€§å¥–åŠ±å‡½æ•°
    
    å¯ä»¥é€šè¿‡åˆ›å»ºè‡ªå®šä¹‰å¥–åŠ±å‡½æ•°æ¥æ”¹å˜è¡Œä¸º
    """
    print("GRPOå¥–åŠ±å‡½æ•°:")
    print("\né»˜è®¤å¥–åŠ±å‡½æ•°: å‡†ç¡®æ€§å¥–åŠ±")
    print("  - ç­”æ¡ˆæ­£ç¡®: 1.0")
    print("  - ç­”æ¡ˆé”™è¯¯: 0.0")
    
    print("\nå…¶ä»–å¯ç”¨å¥–åŠ±å‡½æ•°:")
    print("  1. é•¿åº¦æƒ©ç½šå¥–åŠ±: é¼“åŠ±ç®€æ´ç­”æ¡ˆ")
    print("  2. æ­¥éª¤å¥–åŠ±: é¼“åŠ±è¯¦ç»†æ¨ç†")
    print("  3. è‡ªå®šä¹‰å¥–åŠ±: æ ¹æ®éœ€æ±‚å®šåˆ¶")
    
    print("\nåˆ›å»ºå¥–åŠ±å‡½æ•°ç¤ºä¾‹:")
    tool = RLTrainingTool()
    
    # åˆ›å»ºå‡†ç¡®æ€§å¥–åŠ±å‡½æ•°
    accuracy_config = {
        "action": "create_reward",
        "reward_type": "accuracy"
    }
    print("\n1. å‡†ç¡®æ€§å¥–åŠ±:")
    print(f"   é…ç½®: {accuracy_config}")
    
    # åˆ›å»ºé•¿åº¦æƒ©ç½šå¥–åŠ±å‡½æ•°
    length_config = {
        "action": "create_reward",
        "reward_type": "length_penalty",
        "penalty_weight": 0.001
    }
    print("\n2. é•¿åº¦æƒ©ç½šå¥–åŠ±:")
    print(f"   é…ç½®: {length_config}")
    
    # åˆ›å»ºæ­¥éª¤å¥–åŠ±å‡½æ•°
    step_config = {
        "action": "create_reward",
        "reward_type": "step",
        "step_bonus": 0.1
    }
    print("\n3. æ­¥éª¤å¥–åŠ±:")
    print(f"   é…ç½®: {step_config}")
    
    return accuracy_config, length_config, step_config


# ============================================================================
# ç¤ºä¾‹6: å®é™…è®­ç»ƒç¤ºä¾‹
# ============================================================================

def practical_training_example():
    """
    å®é™…è®­ç»ƒç¤ºä¾‹ - å¯ä»¥ç›´æ¥è¿è¡Œ
    """
    tool = RLTrainingTool()
    
    config = {
        "action": "train",
        "algorithm": "grpo",
        "model_name": "Qwen/Qwen3-0.6B",
        "output_dir": "./output/grpo_practical",
        
        # ä½¿ç”¨è¾ƒå°‘æ ·æœ¬è¿›è¡Œå¿«é€Ÿæµ‹è¯•
        "max_samples": 50,
        "num_epochs": 1,
        "batch_size": 2,
        "learning_rate": 1e-5,
        
        # ä½¿ç”¨LoRA
        "use_lora": True,
        "lora_r": 16,
        "lora_alpha": 32,
    }
    
    print("å®é™…è®­ç»ƒç¤ºä¾‹:")
    print(f"  æ¨¡å‹: {config['model_name']}")
    print(f"  æ ·æœ¬æ•°: {config['max_samples']}")
    print(f"  è®­ç»ƒè½®æ•°: {config['num_epochs']}")
    print(f"  è¾“å‡ºç›®å½•: {config['output_dir']}")
    
    print("\nğŸ’¡ æç¤º: å–æ¶ˆä¸‹é¢çš„æ³¨é‡Šä»¥å¼€å§‹è®­ç»ƒ")
    print("# result = tool.run(config)")
    print("# result_dict = json.loads(result)")
    print("# print(f'âœ… è®­ç»ƒå®Œæˆ! æ¨¡å‹ä¿å­˜åœ¨: {result_dict[\"output_dir\"]}')")
    
    # å®é™…è®­ç»ƒæ—¶å–æ¶ˆæ³¨é‡Š
    # result = tool.run(config)
    # result_dict = json.loads(result)
    # print(f"\nâœ… è®­ç»ƒå®Œæˆ!")
    # print(f"ğŸ“ æ¨¡å‹ä¿å­˜åœ¨: {result_dict['output_dir']}")
    
    return config


# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================

if __name__ == "__main__":
    print("="*80)
    print("ç¤ºä¾‹1: æœ€ç®€å•çš„GRPOè®­ç»ƒ")
    print("="*80)
    minimal_grpo_training()
    
    print("\n" + "="*80)
    print("ç¤ºä¾‹2: æ ‡å‡†GRPOè®­ç»ƒé…ç½®")
    print("="*80)
    standard_grpo_training()
    
    print("\n" + "="*80)
    print("ç¤ºä¾‹3: å®Œæ•´æ•°æ®é›†è®­ç»ƒ")
    print("="*80)
    full_dataset_training()
    
    print("\n" + "="*80)
    print("ç¤ºä¾‹4: SFT + GRPOå®Œæ•´æµç¨‹")
    print("="*80)
    complete_sft_grpo_pipeline()
    
    print("\n" + "="*80)
    print("ç¤ºä¾‹5: ä¸åŒå¥–åŠ±å‡½æ•°çš„ä½¿ç”¨")
    print("="*80)
    using_different_rewards()
    
    print("\n" + "="*80)
    print("ç¤ºä¾‹6: å®é™…è®­ç»ƒç¤ºä¾‹")
    print("="*80)
    practical_training_example()

