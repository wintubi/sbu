"""
ç¤ºä¾‹4: SFTè®­ç»ƒå®Œæ•´æµç¨‹

æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨RLTrainingToolè¿›è¡ŒSFTç›‘ç£å¾®è°ƒ
"""

import sys
from pathlib import Path
import json

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent / "HelloAgents"
sys.path.insert(0, str(project_root))

from hello_agents.tools import RLTrainingTool


# ============================================================================
# ç¤ºä¾‹1: æœ€ç®€å•çš„SFTè®­ç»ƒ
# ============================================================================

def minimal_sft_training():
    """
    æœ€ç®€å•çš„SFTè®­ç»ƒç¤ºä¾‹
    
    åªéœ€è¦è°ƒç”¨RLTrainingToolå³å¯
    """
    tool = RLTrainingTool()
    
    config = {
        "action": "train",
        "algorithm": "sft",
        "model_name": "Qwen/Qwen3-0.6B",
        "output_dir": "./output/sft_minimal",
        "max_samples": 10,
        "num_epochs": 1,
    }
    
    print("æœ€ç®€å•çš„SFTè®­ç»ƒ:")
    print(f"  æ¨¡å‹: {config['model_name']}")
    print(f"  æ ·æœ¬æ•°: {config['max_samples']}")
    print(f"  è®­ç»ƒè½®æ•°: {config['num_epochs']}")
    
    # å®é™…è®­ç»ƒæ—¶å–æ¶ˆæ³¨é‡Š
    # result = tool.run(config)
    # result_dict = json.loads(result)
    # print(f"\nâœ… è®­ç»ƒå®Œæˆ! æ¨¡å‹ä¿å­˜åœ¨: {result_dict['output_dir']}")
    
    return config


# ============================================================================
# ç¤ºä¾‹2: æ ‡å‡†SFTè®­ç»ƒé…ç½®
# ============================================================================

def standard_sft_training():
    """
    æ ‡å‡†çš„SFTè®­ç»ƒé…ç½®
    
    åŒ…å«:
    - LoRAå‚æ•°é«˜æ•ˆå¾®è°ƒ
    - åˆç†çš„è®­ç»ƒå‚æ•°
    - ä½¿ç”¨éƒ¨åˆ†æ•°æ®é›†
    """
    tool = RLTrainingTool()
    
    config = {
        "action": "train",
        "algorithm": "sft",
        
        # æ¨¡å‹é…ç½®
        "model_name": "Qwen/Qwen3-0.6B",
        "output_dir": "./output/sft_standard",
        
        # æ•°æ®é…ç½®
        "max_samples": 1000,  # ä½¿ç”¨1000ä¸ªæ ·æœ¬
        
        # è®­ç»ƒé…ç½®
        "num_epochs": 3,
        "batch_size": 4,
        "learning_rate": 5e-5,
        
        # LoRAé…ç½®
        "use_lora": True,
        "lora_r": 16,
        "lora_alpha": 32,
    }
    
    print("æ ‡å‡†SFTè®­ç»ƒé…ç½®:")
    print(f"  æ¨¡å‹: {config['model_name']}")
    print(f"  æ ·æœ¬æ•°: {config['max_samples']}")
    print(f"  è®­ç»ƒè½®æ•°: {config['num_epochs']}")
    print(f"  batch_size: {config['batch_size']}")
    print(f"  learning_rate: {config['learning_rate']}")
    print(f"  LoRAç§©: {config['lora_r']}")
    
    # å®é™…è®­ç»ƒæ—¶å–æ¶ˆæ³¨é‡Š
    # result = tool.run(config)
    # result_dict = json.loads(result)
    # print(f"\nâœ… è®­ç»ƒå®Œæˆ!")
    # print(f"ğŸ“ æ¨¡å‹ä¿å­˜åœ¨: {result_dict['output_dir']}")
    
    return config


# ============================================================================
# ç¤ºä¾‹3: å®Œæ•´æ•°æ®é›†è®­ç»ƒ
# ============================================================================

def full_dataset_training():
    """
    ä½¿ç”¨å®Œæ•´æ•°æ®é›†è¿›è¡Œè®­ç»ƒ
    
    max_samples=None è¡¨ç¤ºä½¿ç”¨å…¨éƒ¨æ•°æ®
    """
    tool = RLTrainingTool()
    
    config = {
        "action": "train",
        "algorithm": "sft",
        "model_name": "Qwen/Qwen3-0.6B",
        "output_dir": "./output/sft_full",
        
        # ä½¿ç”¨å…¨éƒ¨æ•°æ®
        "max_samples": None,  # None = ä½¿ç”¨å…¨éƒ¨æ•°æ®
        
        "num_epochs": 3,
        "batch_size": 4,
        "learning_rate": 5e-5,
        "use_lora": True,
        "lora_r": 16,
        "lora_alpha": 32,
    }
    
    print("å®Œæ•´æ•°æ®é›†è®­ç»ƒ:")
    print(f"  æ¨¡å‹: {config['model_name']}")
    print(f"  æ ·æœ¬æ•°: å…¨éƒ¨ (max_samples=None)")
    print(f"  è®­ç»ƒè½®æ•°: {config['num_epochs']}")
    print(f"  é¢„è®¡æ ·æœ¬æ•°: ~7500 (GSM8Kè®­ç»ƒé›†)")
    
    # å®é™…è®­ç»ƒæ—¶å–æ¶ˆæ³¨é‡Š
    # result = tool.run(config)
    # result_dict = json.loads(result)
    # print(f"\nâœ… è®­ç»ƒå®Œæˆ!")
    
    return config


# ============================================================================
# ç¤ºä¾‹4: ä¸åŒå­¦ä¹ ç‡çš„å¯¹æ¯”
# ============================================================================

def compare_learning_rates():
    """
    å¯¹æ¯”ä¸åŒå­¦ä¹ ç‡çš„è®­ç»ƒæ•ˆæœ
    
    å¸¸ç”¨å­¦ä¹ ç‡:
    - 1e-5: ä¿å®ˆ,é€‚åˆå¾®è°ƒå·²ç»å¾ˆå¥½çš„æ¨¡å‹
    - 5e-5: æ¨è,å¹³è¡¡å­¦ä¹ é€Ÿåº¦å’Œç¨³å®šæ€§
    - 1e-4: æ¿€è¿›,é€‚åˆå¿«é€Ÿå®éªŒ
    """
    learning_rates = {
        "ä¿å®ˆ (1e-5)": 1e-5,
        "æ¨è (5e-5)": 5e-5,
        "æ¿€è¿› (1e-4)": 1e-4,
    }
    
    print("ä¸åŒå­¦ä¹ ç‡çš„å¯¹æ¯”:")
    for name, lr in learning_rates.items():
        print(f"\n{name}:")
        print(f"  learning_rate: {lr}")
        print(f"  é€‚ç”¨åœºæ™¯: ", end="")
        if lr == 1e-5:
            print("æ¨¡å‹å·²ç»å¾ˆå¥½,åªéœ€å¾®è°ƒ")
        elif lr == 5e-5:
            print("æ ‡å‡†è®­ç»ƒ,æ¨èä½¿ç”¨")
        else:
            print("å¿«é€Ÿå®éªŒ(å¯èƒ½ä¸ç¨³å®š)")
    
    # è®­ç»ƒç¤ºä¾‹
    print("\nè®­ç»ƒç¤ºä¾‹ (æ¨èå­¦ä¹ ç‡):")
    tool = RLTrainingTool()
    config = {
        "action": "train",
        "algorithm": "sft",
        "model_name": "Qwen/Qwen3-0.6B",
        "max_samples": 1000,
        "num_epochs": 3,
        "learning_rate": 5e-5,
        "use_lora": True,
    }
    print(f"  learning_rate: {config['learning_rate']}")
    
    # result = tool.run(config)
    
    return learning_rates


# ============================================================================
# ç¤ºä¾‹5: æ˜¾å­˜ä¼˜åŒ–é…ç½®
# ============================================================================

def memory_optimized_training():
    """
    æ˜¾å­˜ä¼˜åŒ–é…ç½®
    
    é€‚ç”¨äºæ˜¾å­˜å—é™çš„æƒ…å†µ:
    - ä½¿ç”¨LoRA
    - å‡å°batch size
    - ä½¿ç”¨è¾ƒå°çš„LoRAç§©
    """
    tool = RLTrainingTool()
    
    config = {
        "action": "train",
        "algorithm": "sft",
        "model_name": "Qwen/Qwen3-0.6B",
        "output_dir": "./output/sft_memory_opt",
        
        # æ˜¾å­˜ä¼˜åŒ–
        "max_samples": 1000,
        "num_epochs": 3,
        "batch_size": 1,  # æœ€å°batch size
        "learning_rate": 5e-5,
        
        # LoRAé…ç½®
        "use_lora": True,
        "lora_r": 8,  # ä½¿ç”¨è¾ƒå°çš„ç§©
        "lora_alpha": 16,
    }
    
    print("æ˜¾å­˜ä¼˜åŒ–é…ç½®:")
    print(f"  batch_size: {config['batch_size']} (æœ€å°)")
    print(f"  lora_r: {config['lora_r']} (è¾ƒå°)")
    print(f"  use_lora: {config['use_lora']}")
    print(f"  é¢„è®¡æ˜¾å­˜å ç”¨: ~3-4GB")
    
    # å®é™…è®­ç»ƒæ—¶å–æ¶ˆæ³¨é‡Š
    # result = tool.run(config)
    
    return config


# ============================================================================
# ç¤ºä¾‹6: å®é™…è®­ç»ƒç¤ºä¾‹
# ============================================================================

def practical_training_example():
    """
    å®é™…è®­ç»ƒç¤ºä¾‹ - å¯ä»¥ç›´æ¥è¿è¡Œ
    """
    tool = RLTrainingTool()
    
    config = {
        "action": "train",
        "algorithm": "sft",
        "model_name": "Qwen/Qwen3-0.6B",
        "output_dir": "./output/sft_practical",
        
        # ä½¿ç”¨è¾ƒå°‘æ ·æœ¬è¿›è¡Œå¿«é€Ÿæµ‹è¯•
        "max_samples": 100,
        "num_epochs": 1,
        "batch_size": 4,
        "learning_rate": 5e-5,
        
        # ä½¿ç”¨LoRA
        "use_lora": True,
        "lora_r": 16,
        "lora_alpha": 32,
    }
    
    print("å®é™…è®­ç»ƒç¤ºä¾‹:")
    print(f"  æ¨¡å‹: {config['model_name']}")
    print(f"  æ ·æœ¬æ•°: {config['max_samples']}")
    print(f"  è®­ç»ƒè½®æ•°: {config['num_epochs']}")
    print(f"  è¾“å‡ºç›®å½•: {config['output_dir']}")
    
    print("\nğŸ’¡ æç¤º: å–æ¶ˆä¸‹é¢çš„æ³¨é‡Šä»¥å¼€å§‹è®­ç»ƒ")
    print("# result = tool.run(config)")
    print("# result_dict = json.loads(result)")
    print("# print(f'âœ… è®­ç»ƒå®Œæˆ! æ¨¡å‹ä¿å­˜åœ¨: {result_dict[\"output_dir\"]}')")
    
    # å®é™…è®­ç»ƒæ—¶å–æ¶ˆæ³¨é‡Š
    # result = tool.run(config)
    # result_dict = json.loads(result)
    # print(f"\nâœ… è®­ç»ƒå®Œæˆ!")
    # print(f"ğŸ“ æ¨¡å‹ä¿å­˜åœ¨: {result_dict['output_dir']}")
    
    return config


# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================

if __name__ == "__main__":
    print("="*80)
    print("ç¤ºä¾‹1: æœ€ç®€å•çš„SFTè®­ç»ƒ")
    print("="*80)
    minimal_sft_training()
    
    print("\n" + "="*80)
    print("ç¤ºä¾‹2: æ ‡å‡†SFTè®­ç»ƒé…ç½®")
    print("="*80)
    standard_sft_training()
    
    print("\n" + "="*80)
    print("ç¤ºä¾‹3: å®Œæ•´æ•°æ®é›†è®­ç»ƒ")
    print("="*80)
    full_dataset_training()
    
    print("\n" + "="*80)
    print("ç¤ºä¾‹4: ä¸åŒå­¦ä¹ ç‡çš„å¯¹æ¯”")
    print("="*80)
    compare_learning_rates()
    
    print("\n" + "="*80)
    print("ç¤ºä¾‹5: æ˜¾å­˜ä¼˜åŒ–é…ç½®")
    print("="*80)
    memory_optimized_training()
    
    print("\n" + "="*80)
    print("ç¤ºä¾‹6: å®é™…è®­ç»ƒç¤ºä¾‹")
    print("="*80)
    practical_training_example()

